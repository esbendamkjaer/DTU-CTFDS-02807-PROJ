{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only execute this cell once.\n",
    "if '_EXECUTED_' in globals():\n",
    "    # check if variable '_EXECUTED_' exists in the global variable namespace\n",
    "    print(\"Already been executed once, not running again!\")\n",
    "else:\n",
    "    print(\"Cell has not been executed before, running...\")\n",
    "    import os, json, pyspark\n",
    "    from pyspark.conf import SparkConf\n",
    "    from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "    # Two files are automatically read: JobParameters.json for the Spark Cluster job using a temporary spark instance\n",
    "    # and JobParameters.json for the Jupyter Lab job to extract the hostname of the cluster. \n",
    "\n",
    "    MASTER_HOST_NAME = None\n",
    "\n",
    "    # Open the parameters Jupyter Lab app was launched with\n",
    "    with open('/work/JobParameters.json', 'r') as file:\n",
    "        JUPYTER_LAB_JOB_PARAMS = json.load(file)\n",
    "        # from pprint import pprint; pprint(JUPYTER_LAB_JOB_PARAMS) \n",
    "        for resource in JUPYTER_LAB_JOB_PARAMS['request']['resources']:\n",
    "            if 'hostname' in resource.keys():\n",
    "                MASTER_HOST_NAME = resource['hostname']\n",
    "\n",
    "    MASTER_HOST = f\"spark://{MASTER_HOST_NAME}:7077\"\n",
    "\n",
    "    conf = SparkConf().setAll([\n",
    "            (\"spark.app.name\", 'reading_job_params_app'), \n",
    "            (\"spark.master\", MASTER_HOST),\n",
    "        ])\n",
    "    spark = SparkSession.builder.config(conf=conf)\\\n",
    "                                .getOrCreate()\n",
    "\n",
    "    CLUSTER_PARAMETERS_JSON_DF = spark.read.option(\"multiline\",\"true\").json('/work/JobParameters.json')\n",
    "\n",
    "    # Extract cluster info from the specific JobParameters.json\n",
    "    NODES = CLUSTER_PARAMETERS_JSON_DF.select(\"request.replicas\").first()[0]\n",
    "    CPUS_PER_NODE = CLUSTER_PARAMETERS_JSON_DF.select(\"machineType.cpu\").first()[0] - 1\n",
    "    MEM_PER_NODE = CLUSTER_PARAMETERS_JSON_DF.select(\"machineType.memoryInGigs\").first()[0]\n",
    "\n",
    "    CLUSTER_CORES_MAX = CPUS_PER_NODE * NODES\n",
    "    CLUSTER_MEMORY_MAX = MEM_PER_NODE * NODES \n",
    "    \n",
    "    if CPUS_PER_NODE > 1:\n",
    "        EXECUTOR_CORES = CPUS_PER_NODE - 1  # set cores per executor on worker node\n",
    "    else:\n",
    "        EXECUTOR_CORES = CPUS_PER_NODE \n",
    "\n",
    "    EXECUTOR_MEMORY = int(\n",
    "        MEM_PER_NODE / (CPUS_PER_NODE / EXECUTOR_CORES) * 0.5\n",
    "    )  # set executor memory in GB on each worker node\n",
    "\n",
    "    # Make sure there is a dir for spark logs\n",
    "    if not os.path.exists('spark_logs'):\n",
    "        os.mkdir('spark_logs')\n",
    "\n",
    "    conf = SparkConf().setAll(\n",
    "        [\n",
    "            (\"spark.app.name\", 'spark_assignment'), # Change to your liking \n",
    "            (\"spark.sql.caseSensitive\", False), # Optional: Make queries strings sensitive to captialization\n",
    "            (\"spark.master\", MASTER_HOST),\n",
    "            (\"spark.cores.max\", CLUSTER_CORES_MAX),\n",
    "            (\"spark.executor.cores\", EXECUTOR_CORES),\n",
    "            (\"spark.executor.memory\", str(EXECUTOR_MEMORY) + \"g\"),\n",
    "            (\"spark.eventLog.enabled\", True),\n",
    "            (\"spark.eventLog.dir\", \"spark_logs\"),\n",
    "            (\"spark.history.fs.logDirectory\", \"spark_logs\"),\n",
    "            (\"spark.deploy.mode\", \"cluster\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ## check executor memory, taking into accout 10% of memory overhead (minimum 384 MiB)\n",
    "    CHECK = (CLUSTER_CORES_MAX / EXECUTOR_CORES) * (\n",
    "        EXECUTOR_MEMORY + max(EXECUTOR_MEMORY * 0.10, 0.403)\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        int(CHECK) <= CLUSTER_MEMORY_MAX\n",
    "    ), \"Executor memory larger than cluster total memory!\"\n",
    "\n",
    "    # Stop previous session that was just for loading cluster params\n",
    "    spark.stop()\n",
    "\n",
    "    # Start new session with above config, that has better resource handling\n",
    "    spark: SparkSession = SparkSession.builder.config(conf=conf)\\\n",
    "                                .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    _EXECUTED_ = True\n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pyspark\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, BooleanType, LongType, DoubleType, ArrayType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_schema = StructType([\n",
    "    StructField(\"index\", IntegerType(), True),\n",
    "    StructField(\"app_id\", LongType(), True),\n",
    "    StructField(\"app_name\", StringType(), True),\n",
    "    StructField(\"review_id\", LongType(), True),\n",
    "    StructField(\"language\", StringType(), True),\n",
    "    StructField(\"review\", StringType(), True),\n",
    "    StructField(\"timestamp_created\", IntegerType(), True),\n",
    "    StructField(\"timestamp_updated\", IntegerType(), True),\n",
    "    StructField(\"recommended\", BooleanType(), True),\n",
    "    StructField(\"votes_helpful\", IntegerType(), True),\n",
    "    StructField(\"votes_funny\", IntegerType(), True),\n",
    "    StructField(\"weighted_vote_score\", DoubleType(), True),\n",
    "    StructField(\"comment_count\", IntegerType(), True),\n",
    "    StructField(\"steam_purchase\", BooleanType(), True),\n",
    "    StructField(\"received_for_free\", BooleanType(), True),\n",
    "    StructField(\"written_during_early_access\", BooleanType(), True),\n",
    "    StructField(\"author_steamid\", LongType(), True),\n",
    "    StructField(\"author_num_games_owned\", IntegerType(), True),\n",
    "    StructField(\"author_num_reviews\", IntegerType(), True),\n",
    "    StructField(\"author_playtime_forever\", DoubleType(), True),\n",
    "    StructField(\"author_playtime_last_two_weeks\", DoubleType(), True),\n",
    "    StructField(\"author_playtime_at_review\", DoubleType(), True),\n",
    "    StructField(\"author_last_played\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read csv comma separated with specified schema\n",
    "# game_reviews = spark.read.csv('file:////work/ds/steam_reviews_flattened.csv', header=True, schema=review_schema)\n",
    "\n",
    "game_reviews = spark.read.parquet(\"file:////work/ds/steam_reviews_parquet\")\n",
    "game_reviews: DataFrame = game_reviews.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_schema = StructType([\n",
    "    StructField(\"app_id\", LongType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"release_date\", StringType(), True),\n",
    "    StructField(\"genres\", StringType(), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"developer\", StringType(), True),\n",
    "    StructField(\"publisher\", StringType(), True),\n",
    "    StructField(\"original_price\", StringType(), True),\n",
    "    StructField(\"discount_percentage\", StringType(), True),\n",
    "    StructField(\"discounted_price\", StringType(), True),\n",
    "    StructField(\"dlc_available\", IntegerType(), True),\n",
    "    StructField(\"age_rating\", IntegerType(), True),\n",
    "    StructField(\"content_descriptor\", StringType(), True),\n",
    "    StructField(\"about_description\", StringType(), True),\n",
    "    StructField(\"win_support\", BooleanType(), True),\n",
    "    StructField(\"mac_support\", BooleanType(), True),\n",
    "    StructField(\"linux_support\", BooleanType(), True),\n",
    "    StructField(\"awards\", IntegerType(), True),\n",
    "    StructField(\"overall_review\", StringType(), True),\n",
    "    StructField(\"overall_review_percentage\", DoubleType(), True),\n",
    "    StructField(\"overall_review_count\", DoubleType(), True),\n",
    "    StructField(\"recent_review\", StringType(), True),\n",
    "    StructField(\"recent_review_percentage\", DoubleType(), True),\n",
    "    StructField(\"recent_review_count\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "games = spark.read.csv('file:////work/ds/steam-games.csv', header=True, schema=game_schema)\n",
    "games = games.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_reviews.write.parquet(\"file:////work/ds/steam_reviews_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games.write.parquet(\"file:////work/ds/steam_games_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_reviews\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_reviews\\\n",
    "    .select(game_reviews.app_id)\\\n",
    "    .filter(game_reviews.language == 'english')\\\n",
    "    .count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
